# -*- coding: utf-8 -*-
"""maintenance_train.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eOA8tfrCBq8iS-SDVKwjbp2J0rhTodf-
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler # For scaling data
from sklearn.tree import DecisionTreeRegressor # For building ML models
from sklearn.model_selection import train_test_split # For splitting datasets
from sklearn.metrics import (mean_squared_error, # For evaluating ML models
                             mean_absolute_percentage_error,
                             r2_score)
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

df = pd.read_csv('/content/drive/MyDrive/Dataset.csv')

df.head(10)

df.describe()

df.info()

df[['Maintenance_Type', 'Failure_Cause']].describe(include='object')

df.dtypes

"""Data Preprocessing

"""

df.isna().any()

df.duplicated().sum()

"""Univariate Analysis EDA"""

numerical_columns = df.select_dtypes(include=['int64', 'float64']).drop(columns=['Equipment_ID']).columns.tolist()

sns.set_style('whitegrid')

n_plots = len(numerical_columns)

fig, ax = plt.subplots(n_plots, 2, figsize= (10,60))

for i, col in enumerate(numerical_columns):
  sns.histplot(df[col], ax=ax[i][0], kde=True)
  ax[i][0].set_title(f'Histogram of {col}')
  sns.boxplot(df[col], ax=ax[i][1])
  ax[i][1].set_title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()

categorical_columns = ['Maintenance_Type', 'Failure_Cause']

sns.set_style('whitegrid')

fig, axs = plt.subplots(1, 2, figsize=(10, 6))

for col, ax in zip(categorical_columns, axs.flat):
  df[col].value_counts().plot(kind='bar', ax=ax)
  ax.set_title(f'Distribution of {col}')
  ax.set_xlabel(col)
  ax.set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""Bivariate Analysis"""

sns.set_style('whitegrid')

numerical_features = ['Pressure', 'Temperature', 'Vibration', 'Humidity', 'Flow_Rate', 'Power_Consumption', 'Oil_Level', 'Voltage', 'Production_Volume', 'Planned_Downtime_Hours']

fig, axes = plt.subplots(5, 2, figsize=(15, 20))
axes = axes.flatten()

for i, feature in enumerate(numerical_features):
  sns.scatterplot(x=feature, y='Maintenance_Cost', data=df, ax=axes[i])
  axes[i].set_title(f'Plot of {feature} and Maintenance_Cost')

plt.tight_layout()
plt.show()

"""# New section"""

correlaion = df[numerical_columns].corr()

plt.figure(figsize=(15, 10))
sns.heatmap(correlaion, annot=True, fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

sensor_columns = ['Temperature', 'Pressure', 'Vibration', 'Humidity', 'Flow_Rate', 'Power_Consumption', 'Oil_Level', 'Voltage']

plt.figure(figsize=(20, 12))
for i, column in enumerate(sensor_columns, 1):
  plt.subplot(2, 4, i)
  sns.boxplot(x=df['Maintenance_Type'], y=df[column])
  plt.title(f'Boxplot of {column} by Maintenance_Type')
  plt.xlabel('Maintenance_Type')
  plt.ylabel(column)

plt.tight_layout()
plt.show()

plt.figure(figsize=(20, 12))

for i, column in enumerate(sensor_columns, 1):
  plt.subplot(2, 4, i)
  sns.boxplot(x=df['Failure_Cause'], y=df[column])
  plt.title(f'Boxplot of {column} by Failure_Cause')
  plt.xlabel('Failure_Cause')
  plt.ylabel(column)

plt.tight_layout()
plt.show()

print(df[numerical_columns].head())

"""Feature Engineering"""

# Maintenance cost

mc_df = df.drop(columns=['Installation_Date', 'Maintenance_Date', 'Failure_Date', 'Equipment_ID']) #drop those colum as we don't need them
mc_df = pd.get_dummies(mc_df)

mc_X = mc_df.drop(columns=['Maintenance_Cost'])
mc_y = mc_df['Maintenance_Cost']


X_train, X_test, y_train, y_test = train_test_split(mc_X, mc_y, test_size=0.3, random_state=42)

ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""Predict maintenance Cost"""

model = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'XGBoost': XGBRegressor(random_state=42)

}

model_scores = {}

for name, model in model.items():
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  rmse = np.sqrt(mean_squared_error(y_test, y_pred))
  mape = mean_absolute_percentage_error(y_test, y_pred) * 100
  r2 = r2_score(y_test, y_pred)
  model_scores[name] = {'RMSE': rmse, 'MAPE': mape, 'R2': r2}
  print(f'{name} - RMSE: {rmse}, MAPE: {mape}, R2: {r2}')

"""Predict Days Till Failure"""

# Days Till Failure Dataset

dft_df = df.copy().drop(columns=['Equipment_ID', 'Maintenance_Cost'])  #drop column Equipment ID from original dataset
dft_df['Installation_Date'] = pd.to_datetime(dft_df['Installation_Date']) #Convert to date so we can get the differents between each days.
dft_df['Failure_Date'] = pd.to_datetime(dft_df['Failure_Date'])
dft_df['Days_Till_Failure'] = pd.to_timedelta(dft_df['Failure_Date'] - dft_df['Installation_Date']).dt.days # It returns it as days using dt.days

dft_df = dft_df.drop(columns=['Installation_Date', 'Maintenance_Date', 'Failure_Date'])
dft_df = pd.get_dummies(dft_df)

dft_X = dft_df.drop(columns=['Days_Till_Failure']) #Target Variables
dft_y = dft_df['Days_Till_Failure']


X_train, X_test, y_train, y_test = train_test_split(dft_X, dft_y, test_size=0.3, random_state=42)

ss_1 = StandardScaler()
dft_X_train = ss_1.fit_transform(X_train)
dft_X_test = ss_1.transform(X_test)

print(dft_X_train.shape)
print(dft_X_test.shape)
print(y_train.shape)
print(y_test.shape)

dtf_models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'XGBoost': XGBRegressor(random_state=42)

}

dtf_model_scores = {}

for name, model in dtf_models.items():
  model.fit(dft_X_train, y_train)
  y_pred = model.predict(dft_X_test)
  rmse = np.sqrt(mean_squared_error(y_test, y_pred))
  mape = mean_absolute_percentage_error(y_test, y_pred) * 100
  r2 = r2_score(y_test, y_pred)
  dtf_model_scores[name] = {'RMSE': rmse, 'MAPE': mape, 'R2': r2}
  print(f'{name} - RMSE: {rmse}, MAPE: {mape}, R2: {r2}')

#create a pipeline for the best model
dt_pl_mc = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values
    ('scaler', StandardScaler()),                # Scale numerical features
    ('model', LinearRegression())  # LinearRegression
])


#Fit the Pipeline:
#  (Assuming you have 'mc_data' with 'X_train' and 'y_train')
dt_pl_mc.fit(X_train, y_train)

# Make Predictions:
# (Assuming you have 'mc_data' with 'X_test')
y_pred = dt_pl_mc.predict(X_test)

# Evaluate the Model:
# (Assuming you have 'mc_data' with 'y_test')

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mape = mean_absolute_percentage_error(y_test, y_pred) * 100
r2 = r2_score(y_test, y_pred)

print(f'LinearRegression - RMSE: {rmse}, MAPE: {mape}, R2:{r2}')

print(dt_pl_mc)

dt_pl_dtf = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values
    ('scaler', StandardScaler()),                # Scale numerical features
    ('model', LinearRegression())  # LinearRegression
])

#Fit the Pipeline:
#  (Assuming you have 'dft_data' with 'X_train' and 'y_train')
dt_pl_dtf.fit(dft_X_train, y_train)

# Make Predictions:
# (Assuming you have 'mc_data' with 'X_test')
y_pred = dt_pl_dtf.predict(dft_X_test)

# Evaluate the Model:
# (Assuming you have 'mc_data' with 'y_test')

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mape = mean_absolute_percentage_error(y_test, y_pred) * 100
r2 = r2_score(y_test, y_pred)

print(f'LinearRegression - RMSE: {rmse}, MAPE: {mape}, R2:{r2}')
print(dt_pl_dtf)

"""Conclusion."""

#parameters that affect maintenance cost

# Get feature importances from the Decision Tree model in the pipeline
feature_importances = dt_pl_mc.named_steps['model'].coef_


# Create a DataFrame to store feature importances with column names
feature_importance_df = pd.DataFrame({'Feature': mc_X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the top 10 most important features
print(feature_importance_df.head(10))

#parameters that affect maintenance cost

# Get feature importances from the Decision Tree model in the pipeline
feature_importances = dt_pl_dtf.named_steps['model'].coef_

# Create a DataFrame to store feature importances with column names
feature_importance_df = pd.DataFrame({'Feature': dft_X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the top 10 most important features
print(feature_importance_df.head(10))

import joblib

mc_model = {'mc_model': dt_pl_mc}
dtf_model = {'dtf_model_dt': dt_pl_dtf}

joblib.dump(mc_model, 'mc_model.pkl')
joblib.dump(dtf_model, 'dtf_model.pkl')

print(mc_X.columns.tolist())
print(dft_X.columns.tolist())